{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYLTwH9/7tbRHxXEo+xvH1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nimrat4/laplace-smoothing-nlp/blob/main/laplace_smoothing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def tokenize_and_generate_bigrams(input_text):\n",
        "    tokens = re.findall(r'\\b\\w+\\b', input_text.lower())  # Tokenize words using regex\n",
        "    bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]  # Generate bigrams\n",
        "    return tokens, bigrams\n",
        "\n",
        "def calculate_bigram_probabilities_with_pms(input_text, alpha=0.1):\n",
        "    tokens, bigrams = tokenize_and_generate_bigrams(input_text)\n",
        "\n",
        "    # Count occurrences of unigrams and bigrams\n",
        "    unigram_count = defaultdict(int)\n",
        "    bigram_count = defaultdict(int)\n",
        "\n",
        "    for token in tokens:\n",
        "        unigram_count[token] += 1\n",
        "    for bigram in bigrams:\n",
        "        bigram_count[bigram] += 1\n",
        "\n",
        "    # Vocabulary size\n",
        "    vocab_size = len(unigram_count)\n",
        "\n",
        "    # Compute Maximum Likelihood Estimation (MLE) probabilities\n",
        "    bigram_probabilities = {}\n",
        "    for (w1, w2), count in bigram_count.items():\n",
        "        bigram_probabilities[(w1, w2)] = count / unigram_count[w1]\n",
        "\n",
        "    # Apply Probability Mass Stealing (PMS)\n",
        "    bigram_probabilities_pms = {}\n",
        "    for (w1, w2), prob in bigram_probabilities.items():\n",
        "        bigram_probabilities_pms[(w1, w2)] = (1 - alpha) * prob  # Reduce probability of seen bigrams\n",
        "\n",
        "    # Assign probability mass to unseen bigrams\n",
        "    unseen_probability = alpha / vocab_size\n",
        "    for w1 in unigram_count:\n",
        "        for w2 in unigram_count:\n",
        "            if (w1, w2) not in bigram_probabilities_pms:\n",
        "                bigram_probabilities_pms[(w1, w2)] = unseen_probability  # Assign small probability to unseen bigrams\n",
        "\n",
        "    return bigram_probabilities_pms\n",
        "\n",
        "# Given corpus\n",
        "input_text = \"machine learning is amazing. deep learning is powerful. learning is continuous.\"\n",
        "\n",
        "# Compute bigram probabilities using PMS\n",
        "bigram_probabilities_pms = calculate_bigram_probabilities_with_pms(input_text)\n",
        "\n",
        "# Print results\n",
        "print(\"\\nBigram Probabilities (After PMS):\")\n",
        "for bigram, prob in bigram_probabilities_pms.items():\n",
        "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
        "\n",
        "# Compute probability of \"learning is\"\n",
        "target_bigram = (\"learning\", \"is\")\n",
        "print(f\"\\nProbability of 'learning is' after PMS: {bigram_probabilities_pms.get(target_bigram, 0):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2D4bAwIYFbh",
        "outputId": "0d463507-66c5-4246-a1e5-f810e62c7d7f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Probabilities (After PMS):\n",
            "P(learning | machine) = 0.9000\n",
            "P(is | learning) = 0.9000\n",
            "P(amazing | is) = 0.3000\n",
            "P(deep | amazing) = 0.9000\n",
            "P(learning | deep) = 0.9000\n",
            "P(powerful | is) = 0.3000\n",
            "P(learning | powerful) = 0.9000\n",
            "P(continuous | is) = 0.3000\n",
            "P(machine | machine) = 0.0143\n",
            "P(is | machine) = 0.0143\n",
            "P(amazing | machine) = 0.0143\n",
            "P(deep | machine) = 0.0143\n",
            "P(powerful | machine) = 0.0143\n",
            "P(continuous | machine) = 0.0143\n",
            "P(machine | learning) = 0.0143\n",
            "P(learning | learning) = 0.0143\n",
            "P(amazing | learning) = 0.0143\n",
            "P(deep | learning) = 0.0143\n",
            "P(powerful | learning) = 0.0143\n",
            "P(continuous | learning) = 0.0143\n",
            "P(machine | is) = 0.0143\n",
            "P(learning | is) = 0.0143\n",
            "P(is | is) = 0.0143\n",
            "P(deep | is) = 0.0143\n",
            "P(machine | amazing) = 0.0143\n",
            "P(learning | amazing) = 0.0143\n",
            "P(is | amazing) = 0.0143\n",
            "P(amazing | amazing) = 0.0143\n",
            "P(powerful | amazing) = 0.0143\n",
            "P(continuous | amazing) = 0.0143\n",
            "P(machine | deep) = 0.0143\n",
            "P(is | deep) = 0.0143\n",
            "P(amazing | deep) = 0.0143\n",
            "P(deep | deep) = 0.0143\n",
            "P(powerful | deep) = 0.0143\n",
            "P(continuous | deep) = 0.0143\n",
            "P(machine | powerful) = 0.0143\n",
            "P(is | powerful) = 0.0143\n",
            "P(amazing | powerful) = 0.0143\n",
            "P(deep | powerful) = 0.0143\n",
            "P(powerful | powerful) = 0.0143\n",
            "P(continuous | powerful) = 0.0143\n",
            "P(machine | continuous) = 0.0143\n",
            "P(learning | continuous) = 0.0143\n",
            "P(is | continuous) = 0.0143\n",
            "P(amazing | continuous) = 0.0143\n",
            "P(deep | continuous) = 0.0143\n",
            "P(powerful | continuous) = 0.0143\n",
            "P(continuous | continuous) = 0.0143\n",
            "\n",
            "Probability of 'learning is' after PMS: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeSPjuQTUYWb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "def laplace_smoothing_ngram_model(input_text, n_value): #input text and n for gram size- 2for bi 3 for tri\n",
        "    word_tokens = re.findall(r'\\b\\w+\\b', input_text.lower()) #text to lowercase to ensure case senstivity and word extraction\n",
        "    unique_words = set(word_tokens)\n",
        "    vocab_size = len(unique_words)  # Total number of unique words\n",
        "\n",
        "    ngram_frequency = defaultdict(int) #for counting n grams tri int this case\n",
        "    n_minus1_gram_frequency = defaultdict(int) #for counting bi\n",
        "\n",
        "    for i in range(len(word_tokens) - n_value + 1):\n",
        "        current_ngram = tuple(word_tokens[i:i+n_value])  # N-gram 3 in case of trigram , 2 in case of bi gram\n",
        "        current_n_minus1_gram = tuple(word_tokens[i:i+n_value-1])  # (N-1)-gram\n",
        "\n",
        "        ngram_frequency[current_ngram] += 1\n",
        "        n_minus1_gram_frequency[current_n_minus1_gram] += 1\n",
        "\n",
        "    # Computing prob  with Laplace Smoothing\n",
        "    ngram_probabilities = {}\n",
        "\n",
        "    for ngram, count in ngram_frequency.items():\n",
        "        ngram_prefix = ngram[:-1]  # (N-1)-gram\n",
        "        next_word = ngram[-1]  # Last word in the N-gram\n",
        "\n",
        "        # Apply Laplace Smoothing formula\n",
        "        smoothed_probability = (count + 1) / (n_minus1_gram_frequency[ngram_prefix] + vocab_size) # if n=3 P(new word/prefix)= count of n gram+1/count of n-1 gram+vocab size\n",
        "\n",
        "        if ngram_prefix not in ngram_probabilities: #assigns 1/vocab size prob (1/6) default\n",
        "            ngram_probabilities[ngram_prefix] = {}\n",
        "        ngram_probabilities[ngram_prefix][next_word] = smoothed_probability\n",
        "\n",
        "    # Assign probability of 1/V to all unseen words\n",
        "    for ngram_prefix in n_minus1_gram_frequency:\n",
        "        if ngram_prefix not in ngram_probabilities:\n",
        "            ngram_probabilities[ngram_prefix] = {}\n",
        "        for word in unique_words:\n",
        "            if word not in ngram_probabilities[ngram_prefix]:\n",
        "                ngram_probabilities[ngram_prefix][word] = 1 / vocab_size  # Default probability\n",
        "\n",
        "    return ngram_probabilities\n",
        "\n",
        "# Example Usage\n",
        "input_text = \"I love NLP and I love machine learning\"\n",
        "n_value = 3  # Trigram model\n",
        "ngram_probabilities = laplace_smoothing_ngram_model(input_text, n_value)\n",
        "\n",
        "# Print results\n",
        "for ngram_prefix, next_word_probabilities in ngram_probabilities.items():\n",
        "    print(f\"{ngram_prefix}: {dict(next_word_probabilities)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JpPFquSdYBT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}